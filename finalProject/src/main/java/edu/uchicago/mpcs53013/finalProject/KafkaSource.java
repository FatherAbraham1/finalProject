package edu.uchicago.mpcs53013.finalProject;

/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
import scala.Tuple2;
import scala.runtime.AbstractFunction1;

import com.datastax.driver.core.ResultSet;
import com.datastax.driver.core.Row;
import com.datastax.driver.core.Session;
import com.datastax.spark.connector.cql.CassandraConnector;
import com.google.common.collect.Lists;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.StorageLevels;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaInputDStream;
import org.apache.spark.streaming.kafka.KafkaUtils;

import edu.uchicago.mpcs53013.source_new.Source_new;

import java.util.Date;
import java.util.HashMap;
import java.util.regex.Pattern;

import static com.datastax.spark.connector.japi.CassandraJavaUtil.javaFunctions;
import static com.datastax.spark.connector.japi.CassandraJavaUtil.mapToRow;

import org.apache.spark.Logging;
import org.apache.thrift.TDeserializer;
import org.apache.thrift.TException;
import org.apache.thrift.protocol.TJSONProtocol;
import org.apache.log4j.*;
/**
 * Reads real-time thrift encoded WeatherSummaries from a Kafka topic, 
 * and stores the results in Cassandra.
 *
 * Program arguments are Spark master and cassandra address and optionally
 * the Kafka server to connect with (defaults to "localhost:2181")
 */
public final class KafkaSource {
	private static final Pattern SPACE = Pattern.compile(" ");
	static TDeserializer deserializer = new TDeserializer(new TJSONProtocol.Factory());
	public static void main(String[] args) {
		if (args.length < 2) {
			System.err.println("Usage: JavaSource <master> <Cassandra Host> <Optional: Kafka server>");
			System.exit(1);
		}

		boolean log4jInitialized = Logger.getLogger("spark").getAllAppenders().hasMoreElements();
		 if (!log4jInitialized) {
			// We first log something to initialize Spark's default logging, then we override the
			// logging level.
			Logger.getLogger("spark").info("Setting log level to [WARN] for streaming example." +
			" To override add a custom log4j.properties to the classpath.");
			Logger.getLogger("spark").setLevel(Level.WARN);
			Logger.getRootLogger().setLevel(Level.WARN);
			}		
		// Create the context with a 1 second batch size
		SparkConf sparkConf = new SparkConf().setAppName("Source");
	    sparkConf.setMaster(args[0]);
		sparkConf.set("spark.cassandra.connection.host", args[1]);
		JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, new Duration(1000));
		// Create a JavaReceiverInputDStream on target ip:port and count the
		// words in input stream of \n delimited text (eg. generated by 'nc')
		// Note that no duplication in storage level only for running locally.
		// Replication necessary in distributed scenario for fault tolerance.
		JavaPairReceiverInputDStream<String, String> kafkaMessages 
		  = KafkaUtils.createStream(ssc, args.length > 2 ? args[2] : "localhost:2181", "1", new HashMap<String, Integer>() {
			  { put("source-submissions", 1); }
		  });
		JavaDStream<String> lines = kafkaMessages.map(new Function<Tuple2<String, String>, String>() {
			@Override
			public String call(Tuple2<String, String> tuple2) {
				return tuple2._2();
			}
		});
		JavaDStream<CassandraSource> cassandraSource = lines.map(new Function<String, CassandraSource>() {
			@Override
			public CassandraSource call(String x) {
				Source_new source_new = new Source_new();
				try {
					deserializer.fromString(source_new, x);
				} catch (TException e) {
					// TODO Auto-generated catch block
					e.printStackTrace();
				}
				return new CassandraSource(source_new);
			}
		});     
		

		
		javaFunctions(cassandraSource)
        .writerBuilder("wireshark", "source_new", mapToRow(CassandraSource.class))
        .saveToCassandra();

		ssc.start();
		ssc.awaitTermination();
	}
}
